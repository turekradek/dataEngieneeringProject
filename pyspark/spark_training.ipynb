{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n",
    "import findspark\n",
    "import zipfile\n",
    "import os\n",
    "from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"sparksql\").getOrCreate()\n",
    "%load_ext sparksql_magic\n",
    "%config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_files = [\n",
    "    \"2015.zip\",\n",
    "    \"2016.zip\",\n",
    "    \"2017.zip\",\n",
    "    \"2019.zip\",\n",
    "    \"detroit_911_calls.zip\",\n",
    "    \"malaysia_covid_cases.zip\",\n",
    "]\n",
    "state_path = 'DATA/MATURY'\n",
    "unzip_files2 = [ item for item in os.listdir(state_path) if item.endswith('.zip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_file(file_to_unzip):\n",
    "    print(file_to_unzip.split(\".\"))\n",
    "    with zipfile.ZipFile(file_to_unzip, \"r\") as file:\n",
    "        file.extractall(file_to_unzip.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marki =  os.path.join(paths_to_proceed[0], files_to_proceed[0])\n",
    "perfumy =  os.path.join(paths_to_proceed[0], files_to_proceed[1])\n",
    "sklad =  os.path.join(paths_to_proceed[0], files_to_proceed[2])\n",
    "print( marki )\n",
    "print( perfumy )\n",
    "print( sklad )\n",
    "data_marki = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .load(marki)\n",
    ")\n",
    "data_perfumy = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .load(perfumy)\n",
    ")\n",
    "data_sklad = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .load(sklad)\n",
    ")\n",
    "# data_marki = (\n",
    "#     spark.read.text(file1)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
